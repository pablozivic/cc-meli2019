{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction \n",
    "\n",
    "This tutorial presents a walkthrough of the Consensus Categorization (C²) method.\n",
    "\n",
    "The C² method is a supervised and transductive learning method based on the Consensus Clustering method that I investigated during my [doctorate at ICMC-USP](http://www.teses.usp.br/teses/disponiveis/55/55134/tde-05082015-094733/en.php).\n",
    "\n",
    "Here, the C² method has been adapted to handle the dataset provided by [Meli Data Challenge 2019](https://ml-challenge.mercadolibre.com/).\n",
    "\n",
    "In short, the C² method has the following steps:\n",
    "\n",
    "* Preprocess product titles by removing stopwords (English, Portuguese, and Spanish), numbers, and special characters. Source: [meli/preprocess.py](meli/preprocess.py).\n",
    "* Learn a textual representation for product titles by using [fasttext word embeddings](https://fasttext.cc/). Such a representation is useful for initializing classification models.\n",
    "* Get different dataset samples, both by sampling instances and features. Source: [meli/sampling.py](meli/sampling.py)\n",
    "* Get different classification models for each sampling. It is important that there is diversity in classification model solutions. Source: [meli/models.py](meli/models.py)\n",
    "* Build a [heterogeneous network](https://ieeexplore.ieee.org/abstract/document/7536145/) with the following node types: product, terms, and classification models. Some network nodes are labeled considering the training set and the categories predicted by the classification models. The heterogeneous network is regularized through a [consensus function](https://dl.acm.org/citation.cfm?id=2983730) that will return the final categorization. Source: [meli/consensus.py](meli/consensus.py)\n",
    "\n",
    "The C² method ranked fourth (private leaderboard) in the Meli Data Challenge 2019. It can be improved by either adding more classification models or tuning the consensus function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from gensim.test.utils import datapath\n",
    "from gensim.models import KeyedVectors\n",
    "import pandas as pd\n",
    "import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import random\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# code for the Meli Data Challenge\n",
    "from meli import preprocess as pretext\n",
    "from meli import sampling\n",
    "from meli import models\n",
    "from meli import consensus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Dataset Preprocessing\n",
    "\n",
    "* Let's create a directory to store the dataset and then download it from the Meli 2019 repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir dataset\n",
    "!wget -c -P ./dataset/ 'https://meli-data-challenge.s3.amazonaws.com/train.csv.gz'\n",
    "!wget -c -P ./dataset/ 'https://meli-data-challenge.s3.amazonaws.com/test.csv'\n",
    "!cd ./dataset/; gunzip train.csv.gz #To unzip the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Viewing the training set and the test set using pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>label_quality</th>\n",
       "      <th>language</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Hidrolavadora Lavor One 120 Bar 1700w  Bomba A...</td>\n",
       "      <td>unreliable</td>\n",
       "      <td>spanish</td>\n",
       "      <td>ELECTRIC_PRESSURE_WASHERS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Placa De Sonido - Behringer Umc22</td>\n",
       "      <td>unreliable</td>\n",
       "      <td>spanish</td>\n",
       "      <td>SOUND_CARDS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Maquina De Lavar Electrolux 12 Kilos</td>\n",
       "      <td>unreliable</td>\n",
       "      <td>portuguese</td>\n",
       "      <td>WASHING_MACHINES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Par Disco De Freio Diant Vent Gol 8v 08/ Frema...</td>\n",
       "      <td>unreliable</td>\n",
       "      <td>portuguese</td>\n",
       "      <td>VEHICLE_BRAKE_DISCS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Flashes Led Pestañas Luminoso Falso Pestañas P...</td>\n",
       "      <td>unreliable</td>\n",
       "      <td>spanish</td>\n",
       "      <td>FALSE_EYELASHES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19999995</td>\n",
       "      <td>Brochas De Maquillaje Kylie Set De 12 Unidades</td>\n",
       "      <td>unreliable</td>\n",
       "      <td>spanish</td>\n",
       "      <td>MAKEUP_BRUSHES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19999996</td>\n",
       "      <td>Trimmer Detailer Wahl + Kit Tijeras Stylecut</td>\n",
       "      <td>reliable</td>\n",
       "      <td>spanish</td>\n",
       "      <td>HAIR_CLIPPERS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19999997</td>\n",
       "      <td>Bateria Portátil 3300 Mah  Power Bank  Usb Max...</td>\n",
       "      <td>unreliable</td>\n",
       "      <td>portuguese</td>\n",
       "      <td>PORTABLE_CELLPHONE_CHARGERS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19999998</td>\n",
       "      <td>Palo De Hockey Grays Nano 7 37,5''</td>\n",
       "      <td>unreliable</td>\n",
       "      <td>spanish</td>\n",
       "      <td>FIELD_HOCKEY_STICKS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19999999</td>\n",
       "      <td>175x90 Cm Natal Tricotado Sereia Cauda Coberto...</td>\n",
       "      <td>unreliable</td>\n",
       "      <td>portuguese</td>\n",
       "      <td>BEDS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      title label_quality  \\\n",
       "0         Hidrolavadora Lavor One 120 Bar 1700w  Bomba A...    unreliable   \n",
       "1                         Placa De Sonido - Behringer Umc22    unreliable   \n",
       "2                      Maquina De Lavar Electrolux 12 Kilos    unreliable   \n",
       "3         Par Disco De Freio Diant Vent Gol 8v 08/ Frema...    unreliable   \n",
       "4         Flashes Led Pestañas Luminoso Falso Pestañas P...    unreliable   \n",
       "...                                                     ...           ...   \n",
       "19999995     Brochas De Maquillaje Kylie Set De 12 Unidades    unreliable   \n",
       "19999996       Trimmer Detailer Wahl + Kit Tijeras Stylecut      reliable   \n",
       "19999997  Bateria Portátil 3300 Mah  Power Bank  Usb Max...    unreliable   \n",
       "19999998                 Palo De Hockey Grays Nano 7 37,5''    unreliable   \n",
       "19999999  175x90 Cm Natal Tricotado Sereia Cauda Coberto...    unreliable   \n",
       "\n",
       "            language                     category  \n",
       "0            spanish    ELECTRIC_PRESSURE_WASHERS  \n",
       "1            spanish                  SOUND_CARDS  \n",
       "2         portuguese             WASHING_MACHINES  \n",
       "3         portuguese          VEHICLE_BRAKE_DISCS  \n",
       "4            spanish              FALSE_EYELASHES  \n",
       "...              ...                          ...  \n",
       "19999995     spanish               MAKEUP_BRUSHES  \n",
       "19999996     spanish                HAIR_CLIPPERS  \n",
       "19999997  portuguese  PORTABLE_CELLPHONE_CHARGERS  \n",
       "19999998     spanish          FIELD_HOCKEY_STICKS  \n",
       "19999999  portuguese                         BEDS  \n",
       "\n",
       "[20000000 rows x 4 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv('dataset/train.csv')\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Kit Maternidade Bolsa-mala Baby/bebe Vinho Men...</td>\n",
       "      <td>portuguese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Trocador De Fraldas Fisher Price Feminino Rosa...</td>\n",
       "      <td>portuguese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Motor Ventoinha - Fiat Idea / Palio 1.8 - A 04...</td>\n",
       "      <td>portuguese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Amortecedor Mola Batente D Dir New Civic 14 - ...</td>\n",
       "      <td>portuguese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Cadeirinha De Carro Bebê Princesa Princess 9 A...</td>\n",
       "      <td>portuguese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>246950</td>\n",
       "      <td>246950</td>\n",
       "      <td>Disco Freno Delantero Ford Escort 88/94 Nuevo</td>\n",
       "      <td>spanish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>246951</td>\n",
       "      <td>246951</td>\n",
       "      <td>Radio Comunicador Walk Talk Baofeng 777s Profi...</td>\n",
       "      <td>portuguese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>246952</td>\n",
       "      <td>246952</td>\n",
       "      <td>Calculadora De Escritorio Grande 150$</td>\n",
       "      <td>spanish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>246953</td>\n",
       "      <td>246953</td>\n",
       "      <td>Conj Mesa P/ Sala De Jantar C/ 06 Cadeiras Ams...</td>\n",
       "      <td>portuguese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>246954</td>\n",
       "      <td>246954</td>\n",
       "      <td>Cesto Residuos Tacho Basura Automatico 30l + 1...</td>\n",
       "      <td>spanish</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>246955 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                              title    language\n",
       "0            0  Kit Maternidade Bolsa-mala Baby/bebe Vinho Men...  portuguese\n",
       "1            1  Trocador De Fraldas Fisher Price Feminino Rosa...  portuguese\n",
       "2            2  Motor Ventoinha - Fiat Idea / Palio 1.8 - A 04...  portuguese\n",
       "3            3  Amortecedor Mola Batente D Dir New Civic 14 - ...  portuguese\n",
       "4            4  Cadeirinha De Carro Bebê Princesa Princess 9 A...  portuguese\n",
       "...        ...                                                ...         ...\n",
       "246950  246950      Disco Freno Delantero Ford Escort 88/94 Nuevo     spanish\n",
       "246951  246951  Radio Comunicador Walk Talk Baofeng 777s Profi...  portuguese\n",
       "246952  246952              Calculadora De Escritorio Grande 150$     spanish\n",
       "246953  246953  Conj Mesa P/ Sala De Jantar C/ 06 Cadeiras Ams...  portuguese\n",
       "246954  246954  Cesto Residuos Tacho Basura Automatico 30l + 1...     spanish\n",
       "\n",
       "[246955 rows x 3 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv('dataset/test.csv')\n",
    "df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let's do a simple preprocessing of text in the product titles: stopwords removal (English, Portuguese and Spanish) and special characters removal. Use the clean_text() function available in [meli/preprocess.py](meli/preprocess.py). This operation may take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas() \n",
    "\n",
    "df_train['title_clean'] = df_train.progress_apply(lambda x: pretext.clean_text(x['title'], x['language']), axis=1)\n",
    "df_test['title_clean'] = df_test.progress_apply(lambda x: pretext.clean_text(x['title'], x['language']), axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* I used the pickle library to save a binary version of the preprocessed dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(df_train, open('./dataset/df_train.pd', 'wb'))\n",
    "pickle.dump(df_test, open('./dataset/df_test.pd', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload here if the dataset has already been preprocessed.\n",
    "#df_train = pickle.load( open( \"./dataset/df_train.pd\", \"rb\" ) )\n",
    "#df_test = pickle.load( open( \"./dataset/df_test.pd\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>title_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Hidrolavadora Lavor One 120 Bar 1700w  Bomba A...</td>\n",
       "      <td>hidrolavadora lavor one 120bar 1700w bomba alu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Placa De Sonido - Behringer Umc22</td>\n",
       "      <td>placa sonido behringer umc22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Maquina De Lavar Electrolux 12 Kilos</td>\n",
       "      <td>maquina lavar electrolux kilos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Par Disco De Freio Diant Vent Gol 8v 08/ Frema...</td>\n",
       "      <td>par disco freio diant vent gol 8v fremax bd5298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Flashes Led Pestañas Luminoso Falso Pestañas P...</td>\n",
       "      <td>flashes led pestanas luminoso falso pestanas p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19999995</td>\n",
       "      <td>Brochas De Maquillaje Kylie Set De 12 Unidades</td>\n",
       "      <td>brochas maquillaje kylie set unidades</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19999996</td>\n",
       "      <td>Trimmer Detailer Wahl + Kit Tijeras Stylecut</td>\n",
       "      <td>trimmer detailer wahl kit tijeras stylecut</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19999997</td>\n",
       "      <td>Bateria Portátil 3300 Mah  Power Bank  Usb Max...</td>\n",
       "      <td>bateria portatil 3300mah power bank usb maxprint</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19999998</td>\n",
       "      <td>Palo De Hockey Grays Nano 7 37,5''</td>\n",
       "      <td>palo hockey grays nano</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19999999</td>\n",
       "      <td>175x90 Cm Natal Tricotado Sereia Cauda Coberto...</td>\n",
       "      <td>175x90 cm natal tricotado sereia cauda coberto...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      title  \\\n",
       "0         Hidrolavadora Lavor One 120 Bar 1700w  Bomba A...   \n",
       "1                         Placa De Sonido - Behringer Umc22   \n",
       "2                      Maquina De Lavar Electrolux 12 Kilos   \n",
       "3         Par Disco De Freio Diant Vent Gol 8v 08/ Frema...   \n",
       "4         Flashes Led Pestañas Luminoso Falso Pestañas P...   \n",
       "...                                                     ...   \n",
       "19999995     Brochas De Maquillaje Kylie Set De 12 Unidades   \n",
       "19999996       Trimmer Detailer Wahl + Kit Tijeras Stylecut   \n",
       "19999997  Bateria Portátil 3300 Mah  Power Bank  Usb Max...   \n",
       "19999998                 Palo De Hockey Grays Nano 7 37,5''   \n",
       "19999999  175x90 Cm Natal Tricotado Sereia Cauda Coberto...   \n",
       "\n",
       "                                                title_clean  \n",
       "0         hidrolavadora lavor one 120bar 1700w bomba alu...  \n",
       "1                              placa sonido behringer umc22  \n",
       "2                            maquina lavar electrolux kilos  \n",
       "3           par disco freio diant vent gol 8v fremax bd5298  \n",
       "4         flashes led pestanas luminoso falso pestanas p...  \n",
       "...                                                     ...  \n",
       "19999995              brochas maquillaje kylie set unidades  \n",
       "19999996         trimmer detailer wahl kit tijeras stylecut  \n",
       "19999997   bateria portatil 3300mah power bank usb maxprint  \n",
       "19999998                             palo hockey grays nano  \n",
       "19999999  175x90 cm natal tricotado sereia cauda coberto...  \n",
       "\n",
       "[20000000 rows x 2 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[['title','title_clean']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Representation Learning\n",
    "\n",
    "Some classification models may benefit from using word embeddings. I used fasttext to create word vectors from product titles.\n",
    "\n",
    "* Download and compile fasttext (compilation environment required)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://github.com/facebookresearch/fastText/archive/v0.9.1.zip\n",
    "!unzip v0.9.1.zip\n",
    "!cd fastText-0.9.1; make # You must have a compatible compiler (eg debian or ubuntu build-essentials package)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Train word representation with skip-gram from product titles.\n",
    "* The word embeddings file (product_titles.vec) will be saved in ./datasets directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Saving product titles...')\n",
    "df_product_titles = pd.concat([ df_train[['title_clean']] , df_test[['title_clean']] ])\n",
    "df_product_titles.to_csv('./dataset/product_titles.txt',index=False,header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Learning word embeddings')\n",
    "!cd fastText-0.9.1; ./fasttext skipgram -dim 300 -minCount 2 -wordNgrams 2 -minn 0 -maxn 0 -input ../dataset/product_titles.txt -output ../dataset/product_titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let's check our word embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading word embeddings model...\n",
      "Loading word embeddings model... OK\n",
      "Words that are similar to samsung.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('sansung', 0.8280077576637268),\n",
       " ('samsumg', 0.8134722709655762),\n",
       " ('sansumg', 0.7443801164627075),\n",
       " ('samung', 0.7332087755203247),\n",
       " ('samgung', 0.7327459454536438),\n",
       " ('samsug', 0.723293662071228),\n",
       " ('samsun', 0.7043582797050476),\n",
       " ('samnsung', 0.6975691914558411),\n",
       " ('smasung', 0.6821693181991577),\n",
       " ('s5367', 0.6568795442581177)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Loading word embeddings model...')\n",
    "title_embedding = KeyedVectors.load_word2vec_format('./dataset/product_titles.vec', binary=False)\n",
    "print('Loading word embeddings model... OK')\n",
    "\n",
    "word = 'samsung'\n",
    "print('Words that are similar to samsung.')\n",
    "title_embedding.most_similar('samsung')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words that are similar to smartphone.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('celular', 0.8157174587249756),\n",
       " ('smartphones', 0.6840193271636963),\n",
       " ('smartpho', 0.6817572712898254),\n",
       " ('smarthphone', 0.6655287742614746),\n",
       " ('smartfones', 0.6596484184265137),\n",
       " ('celulares', 0.6565076112747192),\n",
       " ('smartphon', 0.6550799012184143),\n",
       " ('ceular', 0.6521972417831421),\n",
       " ('leeremi', 0.6448410749435425),\n",
       " ('jmxl7', 0.644024133682251)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Words that are similar to smartphone.')\n",
    "title_embedding.most_similar('smartphone')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Saving word embedding index in a binary format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "with open('./dataset/product_titles.vec') as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, 'f', sep=' ')\n",
    "        embeddings_index[word] = coefs\n",
    "        \n",
    "\n",
    "\n",
    "pickle.dump(embeddings_index, open('./dataset/product_titles.index.vec', 'wb'))\n",
    "\n",
    "# embeddings_index = pickle.load( open( \"./dataset/product_titles.index.vec\", \"rb\" ) )  # Use this if embedding has already been generated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Learning the classification models from sampling\n",
    "\n",
    "Sampling was performed in two ways: balanced and unbalanced. In balanced sampling, the same number of instances per class (when possible) was sampled. In addition, instances marked as \"reliables\" have higher priority in balanced sampling. In unbalanced sampling, instances are obtained randomly, but with at least one instance per class.\n",
    "\n",
    "For each sampling three different neural network models are trained: LSTM, GRU and CNN. The parameters of each model are set randomly to increase solution diversity.\n",
    "\n",
    "Each model is initially trained in balanced sampling. Another model is obtained by continuing the previous training, but by inserting unbalanced sampling. This process is repeated over a number of k different samplings. I used k = 20 in Meli Data Challenge 2019, thereby generating a total of 120 models.\n",
    "\n",
    "This step can be adapted to run in parallel if you have multiple GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=20  # Tip: Use a low value of k for testing purposes.\n",
    "\n",
    "!mkdir models # dir to save classification models and prediction data.\n",
    "\n",
    "epochs = [5,7,10,15]\n",
    "batch_size = [64,128,256,512] # Configure according to your hardware capacity.\n",
    "\n",
    "for num_iter in range(0,k): # number of balanced samplings \n",
    "    \n",
    "    print('Iteration '+str((num_iter+1))+'/'+str(k))\n",
    "    \n",
    "    # get balanced sampling\n",
    "    print('Getting balanced sampling...')\n",
    "    df_train_balanced_sampling = sampling.balanced_sampling(df_train)\n",
    "    #df_train_balanced_sampling = pickle.load( open( \"./dataset/df_train_balanced_sampling.pd\", \"rb\" ) ) \n",
    "    \n",
    "    # data tokenization for neural network\n",
    "    print('Data tokenization...')\n",
    "    tokenizer, embedding_matrix, MAX_NB_WORDS, MAX_SEQUENCE_LENGTH, nb_words = models.data_input(df_train_balanced_sampling, embeddings_index)\n",
    "    \n",
    "    # shuffle data\n",
    "    df_train_balanced_sampling = df_train_balanced_sampling.sample(frac=1)\n",
    "\n",
    "    # get labels\n",
    "    labels = pd.get_dummies(df_train_balanced_sampling['category'])\n",
    "    Y = labels.values\n",
    "    number_of_classes = Y.shape[1]\n",
    "\n",
    "    # tokenize X training data\n",
    "    X = tokenizer.texts_to_sequences(df_train_balanced_sampling['title_clean'].apply(str))\n",
    "    X = keras.preprocessing.sequence.pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.10)\n",
    "    print('X_train and Y_train shapes:')\n",
    "    print(X_train.shape,Y_train.shape)\n",
    "    print('X_test and Y_test shapes:')\n",
    "    print(X_test.shape,Y_test.shape)\n",
    "    \n",
    "    # codes for categoies\n",
    "    print('Generating category codes...')\n",
    "    category_index = {}\n",
    "    total_categories = len(df_train_balanced_sampling['category'].unique())\n",
    "    for i in range(0,len(Y)):\n",
    "      category_index[np.argmax(Y[i])] = df_train_balanced_sampling.iloc[i,3]\n",
    "      if len(category_index)==total_categories: break\n",
    "\n",
    "    # tokenize Z test data\n",
    "    Z = tokenizer.texts_to_sequences(df_test['title_clean'].apply(str))\n",
    "    Z = keras.preprocessing.sequence.pad_sequences(Z, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    \n",
    "    # generate models (neural networks) using random parameters\n",
    "    model_dic = {}\n",
    "    print('GRU Model..')\n",
    "    model_dic['GRU'] = models.TextGRU(nb_words, MAX_SEQUENCE_LENGTH, 300, embedding_matrix, number_of_classes)\n",
    "    print('LSTM Model..')\n",
    "    model_dic['LSTM'] = models.TextLSTM(nb_words, MAX_SEQUENCE_LENGTH, 300, embedding_matrix, number_of_classes)\n",
    "    print('CNN Model..')\n",
    "    model_dic['CNN'] = models.TextCNN(nb_words, MAX_SEQUENCE_LENGTH, 300, embedding_matrix, number_of_classes)\n",
    "    \n",
    "    \n",
    "    # training neural network from balanced dataset\n",
    "    for model_name in model_dic:\n",
    "        r = random.randrange(100000, 999999)\n",
    "        model_file = 'models/'+model_name+'_'+str(r)+\".v1.model\"\n",
    "        print('Learning classification model '+model_file)\n",
    "        model = model_dic[model_name]\n",
    "        model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=random.choice(epochs), batch_size=random.choice(batch_size), callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])\n",
    "        model.save(model_file)\n",
    "        pickle.dump(tokenizer, open(model_file+'.tokenizer', 'wb'))\n",
    "\n",
    "        preds = model.predict(Z)\n",
    "\n",
    "        probs = []\n",
    "        labels_preds = []\n",
    "        confidences = []\n",
    "        counter = 0\n",
    "        for index,row in df_test.iterrows():\n",
    "          pred = preds[counter]\n",
    "          confidence = np.max(pred)\n",
    "          label_pred = category_index[np.argmax(pred)]\n",
    "          probs.append(pred)\n",
    "          labels_preds.append(label_pred)\n",
    "          confidences.append(confidence)\n",
    "          counter+=1\n",
    "\n",
    "        df_test['probs']=probs\n",
    "        df_test['confidence']=confidences\n",
    "        df_test['label']=labels_preds\n",
    "\n",
    "        df_test.to_csv(model_file+'.test.csv') # save model predictions\n",
    "        \n",
    "    \n",
    "    # get unbalanced sampling\n",
    "    df_train_unbalanced_sampling = sampling.unbalanced_sampling(df_train)\n",
    "\n",
    "    labels = pd.get_dummies(df_train_unbalanced_sampling['category'])\n",
    "    Y = labels.values\n",
    "    number_of_classes = Y.shape[1]\n",
    "\n",
    "    X = tokenizer.texts_to_sequences(df_train_unbalanced_sampling['title_clean'].apply(str))\n",
    "    X = keras.preprocessing.sequence.pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.10)\n",
    "    print(X_train.shape,Y_train.shape)\n",
    "    print(X_test.shape,Y_test.shape)\n",
    "    \n",
    "    # training neural network from unbalanced dataset \n",
    "    for model_name in model_dic:\n",
    "        r = random.randrange(100000, 999999)\n",
    "        model_file = 'models/'+model_name+'_'+str(r)+\".v2.model\"\n",
    "        print('Learning classification model '+model_file)\n",
    "        model = model_dic[model_name]\n",
    "        model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=random.choice(epochs), batch_size=random.choice(batch_size), callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])\n",
    "        model.save(model_file)\n",
    "        pickle.dump(tokenizer, open(model_file+'.tokenizer', 'wb'))\n",
    "\n",
    "\n",
    "        preds = model.predict(Z)\n",
    "\n",
    "        probs = []\n",
    "        labels_preds = []\n",
    "        confidences = []\n",
    "        counter = 0\n",
    "        for index,row in df_test.iterrows():\n",
    "          pred = preds[counter]\n",
    "          confidence = np.max(pred)\n",
    "          label_pred = category_index[np.argmax(pred)]\n",
    "          probs.append(pred)\n",
    "          labels_preds.append(label_pred)\n",
    "          confidences.append(confidence)\n",
    "          counter+=1\n",
    "\n",
    "        df_test['probs']=probs\n",
    "        df_test['confidence']=confidences\n",
    "        df_test['label']=labels_preds\n",
    "\n",
    "        df_test.to_csv(model_file+'.test.csv')# save model predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Consensus Function using Transductive Learning\n",
    "\n",
    "Here, training data and model predictions are combined into a heterogeneous network. The consensus function is a network regularization through label propagation, where the target nodes are the test instances.\n",
    "\n",
    "* If many models agree on the category of a test instance, then there is a good chance that this category will be maintained during regularization.\n",
    "\n",
    "* If most models are in disagreement, the consensus function tends to identify and disable the importance of weak models. Moreover, consensus function use more training data to identify the final product category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Here the predictions of each model are summarized. In Meli Data Challenge, predictions with confidence greater than or equal to 0.9 had higher priority. It was also ensured that each test instance had at least 5 predictions from different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_data = consensus.get_prediction_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Generation of heterogeneous network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_balanced_sampling = sampling.balanced_sampling(df_train)\n",
    "G, label_to_code, code_to_label = consensus.generate_network(df_train_balanced_sampling, df_test, prediction_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The regularization process to identify a consensus is carried out below. This operation may take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = consensus.regularization(G, code_to_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* After regularization, save the final consensus prediction (categories) for submission in Meli 2019 (file: meli_submission.csv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "labels_consensus = []\n",
    "for index,row in df_test.iterrows():\n",
    "    f = G.nodes[str(index)+':doc_test']['f']\n",
    "    label = code_to_label[np.argmax(f)]\n",
    "    labels_consensus.append(label)\n",
    "\n",
    "df_test['category'] = labels_consensus\n",
    "\n",
    "df_test[['id','category']].to_csv('meli_submission.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
